version: '3.8'

services:
  # Redis service for caching
  redis:
    image: redis:7-alpine
    container_name: rag_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - rag_network

  # Ollama service for local LLM with GPU acceleration (COMMENTED OUT - NOW USING CHATGPT/OPENAI)
  # Uncomment to use Ollama with llama3.1:8b + gemma2:300m
  # GPU auto-detected: NVIDIA (CUDA), AMD (ROCm), Apple (Metal)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: rag_ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   networks:
  #     - rag_network
  #   # GPU Configuration (for NVIDIA GPUs with docker-compose)
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   # For AMD GPUs: use 'device: /dev/kfd' and 'device: /dev/dri'
  #   # For Apple Silicon: GPU (Metal) is used automatically, no config needed
  #   healthcheck:
  #     test: ["CMD", "ollama", "list"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 10s
  #   # Pull models on container start
  #   command: >
  #     sh -c "ollama serve &
  #            sleep 5 &&
  #            ollama pull llama3.1:8b &&
  #            ollama pull gemma2:300m &&
  #            wait"

  # ROS2 RAG Service
  rag_service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag_service_node
    depends_on:
      redis:
        condition: service_healthy
      # ollama:  # COMMENTED OUT - NOW USING CHATGPT/OPENAI
      #   condition: service_healthy
    environment:
      # OLLAMA CONFIGURATION (COMMENTED OUT - NOW USING CHATGPT/OPENAI)
      # Uncomment for llama3.1:8b + gemma2:300m with GPU
      # - OLLAMA_BASE_URL=http://ollama:11434
      # - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:8b}
      # - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-gemma2:300m}
      - OPENAI_API_KEY=${OPENAI_API_KEY}  # ChatGPT/OpenAI API Key
      - REDIS_URL=redis://redis:6379
      - ROS_DOMAIN_ID=${ROS_DOMAIN_ID:-0}
    volumes:
      # Mount the package sources for development (optional)
      - ./rag_interfaces:/ros2_ws/src/rag_interfaces
      - ./rag_service:/ros2_ws/src/rag_service
      # Persist vector databases
      - vector_dbs:/ros2_ws/src/rag_service/data/vector_dbs
    networks:
      - rag_network
    stdin_open: true
    tty: true
    command: ros2 launch rag_service rag_service_launch.py

  # Optional: Example client for testing
  rag_client:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag_client
    depends_on:
      - rag_service
    environment:
      - ROS_DOMAIN_ID=0
    networks:
      - rag_network
    stdin_open: true
    tty: true
    command: ros2 run rag_service rag_client_example
    profiles:
      - client  # Only start with: docker-compose --profile client up

volumes:
  redis_data:
    driver: local
  vector_dbs:
    driver: local
  # Uncomment for Ollama (stores models: llama3.1:8b ~5GB, gemma2:300m ~500MB)
  # ollama_data:
  #   driver: local

networks:
  rag_network:
    driver: bridge
